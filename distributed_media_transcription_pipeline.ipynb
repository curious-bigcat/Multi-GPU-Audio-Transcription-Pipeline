{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9736dc-2c27-4e68-bdae-ebb724531507",
   "metadata": {
    "collapsed": false,
    "name": "intro"
   },
   "source": [
    "# Distributed Multi-Node, Multi-GPU Audio Transcription in ML Container Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_libs"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import torch\n",
    "# We can also use Snowpark for our analyses!\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import shutil\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.ml.ray.datasource import SFStageBinaryFileDataSource\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from snowflake.ml.runtime_cluster import scale_cluster, get_nodes\n",
    "from snowflake.ml.ray.datasink import SnowflakeTableDatasink\n",
    "import ray\n",
    "import subprocess\n",
    "import logging\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae6dba-d680-4451-bd96-c69af5e63bb6",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "create or replace stage AUDIO_FILES_STAGE ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') DIRECTORY = (ENABLE = TRUE);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8fee7-435d-4afc-81e2-dbabc0a829e0",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "# UPLOAD ALL THE VIDEO FILES TO ABOVE STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608ae08-45bd-496e-96fe-1d989ba307c1",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# --- Local directories for downloads and converted files ---\n",
    "download_dir = \"/tmp/mp4_downloads\"\n",
    "mp3_dir = \"/tmp/mp3_outputs\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "os.makedirs(mp3_dir, exist_ok=True)\n",
    "\n",
    "# --- List all MP4 files in the stage ---\n",
    "file_list = session.sql(\n",
    "    \"LIST @MULTINODE_MULTIGPU_MYDB.AUDIO_TRANSCRIPTION_SCH.AUDIO_FILES_STAGE PATTERN='.*\\\\.mp4';\"\n",
    ").to_pandas()\n",
    "\n",
    "print(\"Columns:\", file_list.columns.tolist())\n",
    "if not file_list.empty:\n",
    "    print(\"Sample row:\", file_list.iloc[0])\n",
    "\n",
    "# --- Download MP4 files locally ---\n",
    "for _, row in file_list.iterrows():\n",
    "    stage_path = row.iloc[0]  # first column contains the stage path\n",
    "    local_mp4 = os.path.join(download_dir, os.path.basename(stage_path))\n",
    "    session.file.get(f\"@{stage_path}\", download_dir)  # no overwrite arg\n",
    "    print(f\"Downloaded {stage_path} to {local_mp4}\")\n",
    "\n",
    "# --- Convert each MP4 to MP3 ---\n",
    "for mp4_file in glob.glob(os.path.join(download_dir, \"*.mp4\")):\n",
    "    mp3_file = os.path.join(\n",
    "        mp3_dir,\n",
    "        os.path.splitext(os.path.basename(mp4_file))[0] + \".mp3\"\n",
    "    )\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\", \"-y\", \"-i\", mp4_file, \"-vn\", \"-acodec\", \"libmp3lame\",\n",
    "        \"-ar\", \"44100\", \"-ac\", \"2\", \"-b:a\", \"192k\", mp3_file\n",
    "    ], check=True)\n",
    "    print(f\"Converted {mp4_file} to {mp3_file}\")\n",
    "\n",
    "# --- Upload MP3 files back to the stage ---\n",
    "for mp3_file in glob.glob(os.path.join(mp3_dir, \"*.mp3\")):\n",
    "    put_result = session.file.put(\n",
    "        mp3_file,\n",
    "        \"@MULTINODE_MULTIGPU_MYDB.AUDIO_TRANSCRIPTION_SCH.AUDIO_FILES_STAGE\",\n",
    "        auto_compress=False,\n",
    "        overwrite=True\n",
    "    )\n",
    "    print(f\"Uploaded {mp3_file} to stage: {put_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55da4f5-449a-4c69-9ef0-3a5f70421e17",
   "metadata": {
    "collapsed": false,
    "name": "intro_see_starter_nodes"
   },
   "source": [
    "### Start with one node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ce5cd-c2b0-41fa-bcec-4a77fda4cf0e",
   "metadata": {
    "language": "python",
    "name": "initialize_ray_and_start"
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "num_nodes = len([node for node in ray.nodes() if node[\"Alive\"] == True])\n",
    "print(num_nodes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11d409-2cb0-4107-902b-502699d54fe5",
   "metadata": {
    "collapsed": false,
    "name": "intro_scale_to_5_nodes"
   },
   "source": [
    "### Scale up to 5 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efebb414-e8f2-46ee-adda-173da0ced783",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "scale_to_5_nodes"
   },
   "outputs": [],
   "source": [
    "# Asynchronous scaling - function returns immediately after request is accepted\n",
    "scale_cluster(5, is_async=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66019ea-840e-490b-bc91-30112590595f",
   "metadata": {
    "collapsed": false,
    "name": "intro_control_ray_logging"
   },
   "source": [
    "### Control ray logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5778d7-1e10-4831-9757-1410fdc7a383",
   "metadata": {
    "language": "python",
    "name": "control_ray_logging"
   },
   "outputs": [],
   "source": [
    "def configure_ray_logger() -> None:\n",
    "    #Configure Ray logging\n",
    "    ray_logger = logging.getLogger(\"ray\")\n",
    "    ray_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    data_logger = logging.getLogger(\"ray.data\")\n",
    "    data_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "    #Configure Ray's data context\n",
    "    context = ray.data.DataContext.get_current()\n",
    "    context.execution_options.verbose_progress = False\n",
    "    context.enable_operator_progress_bars = False\n",
    "\n",
    "configure_ray_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee4cc4-a6eb-443a-a34c-66fddbfcc151",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "install_ffmpeg_each_node"
   },
   "outputs": [],
   "source": [
    "! ffmpeg -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abca883-cf62-4603-ad55-3273b4c0806b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "print_gpus_in_ray_cluster"
   },
   "outputs": [],
   "source": [
    "print(int(ray.cluster_resources()['GPU']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d5937-05b5-473a-b3b3-b58d3e5c998a",
   "metadata": {
    "collapsed": false,
    "name": "intro_see_audio_files"
   },
   "source": [
    "# See audio & video files in snowflake stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938569bd-67ca-4657-b922-b126aef46f91",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "see_audio_files"
   },
   "outputs": [],
   "source": [
    "ls @AUDIO_FILES_STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5905b6-5665-4c25-a44f-747b3b86fd7f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "get_ray_dataset_using_snow_apis"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.ray.datasource import SFStageBinaryFileDataSource\n",
    "\n",
    "audio_source = SFStageBinaryFileDataSource(\n",
    "    stage_location=\"@AUDIO_FILES_STAGE\",\n",
    "    database=\"MULTINODE_MULTIGPU_MYDB\",\n",
    "    schema=\"AUDIO_TRANSCRIPTION_SCH\",\n",
    "    file_pattern=\"*.mp3\"\n",
    ")\n",
    "\n",
    "audio_dataset = ray.data.read_datasource(audio_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5462d-e0e4-4396-b383-1c7e1f7645b9",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "see_count_of_audio_files"
   },
   "outputs": [],
   "source": [
    "audio_dataset.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcd965-ea6d-4850-9786-44c274495bd3",
   "metadata": {
    "collapsed": false,
    "name": "intro_get_whisper_model"
   },
   "source": [
    "### Get whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1605a95-9fc0-4511-ae2b-7343208a242a",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "set_model_params"
   },
   "outputs": [],
   "source": [
    "model_id = \"openai/whisper-large-v3\"\n",
    "batch_size = 30\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda_available else \"cpu\")\n",
    "torch_dtype = torch.float16 if is_cuda_available else torch.float32\n",
    "print(device)\n",
    "print(torch_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ddbbe-3cdc-4964-91ef-8c316dab899e",
   "metadata": {
    "collapsed": false,
    "name": "intro_distributed_inferencing"
   },
   "source": [
    "### Distributed inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b9201-d3e2-492b-b525-b78335d57a4f",
   "metadata": {
    "language": "python",
    "name": "audio_transcription_class"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "class TranscribeAudioUpdated:\n",
    "    def __init__(self):\n",
    "        # initialize model here so that model can be put into correct GPU/node\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "        )\n",
    "        model.to(device)\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "        self.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            max_new_tokens=128,\n",
    "            chunk_length_s=30,\n",
    "            batch_size=batch_size,\n",
    "            return_timestamps=True,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=device,\n",
    "            generate_kwargs={\"language\": \"hindi\"}\n",
    "        )\n",
    "\n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        temp_files = []\n",
    "        try:\n",
    "            # Write each binary to a temporary file.\n",
    "            for binary_content in batch[\"file_binary\"]:\n",
    "                # Use an appropriate suffix (e.g., .mp3) based on your audio format.\n",
    "                tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "                tmp_file.write(binary_content)\n",
    "                tmp_file.close()\n",
    "                temp_files.append(tmp_file.name)\n",
    "            \n",
    "            # Use the temporary file paths for inference.\n",
    "            predictions = self.pipe(temp_files)\n",
    "            assert len(predictions) == len(batch)\n",
    "            outputs = [str(generated_audio[\"text\"]).strip() for generated_audio in predictions]\n",
    "            batch['outputs'] = outputs\n",
    "            batch.drop(columns=['file_binary'], inplace=True)\n",
    "        finally:\n",
    "            # Clean up temporary files.\n",
    "            for file_path in temp_files:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except OSError:\n",
    "                    pass\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1dec8-2eec-4874-93b2-7771b973f0f4",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "map_batches_of_audio_files"
   },
   "outputs": [],
   "source": [
    "transcribed_ds = audio_dataset.map_batches(TranscribeAudioUpdated,\n",
    "        batch_size=batch_size,\n",
    "        batch_format='pandas',\n",
    "        concurrency=5,\n",
    "        num_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff09d59-99ab-4ca7-9d27-bb17133dcb59",
   "metadata": {
    "language": "sql",
    "name": "drop_results_table_if_exists"
   },
   "outputs": [],
   "source": [
    "drop table if exists WHISPER_DEMO_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b998b9-ea51-4536-9da2-9539f3615f4d",
   "metadata": {
    "language": "python",
    "name": "output_to_a_snowflake_table"
   },
   "outputs": [],
   "source": [
    "datasink = SnowflakeTableDatasink(\n",
    "    table_name=\"WHISPER_DEMO_OUTPUT\",\n",
    "    database=session.get_current_database(),\n",
    "    schema=session.get_current_schema(),\n",
    "    auto_create_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e0495-e8aa-483e-8bff-d7797e099afa",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "write_transcriptions_to_snowflake"
   },
   "outputs": [],
   "source": [
    "transcribed_ds.write_datasink(datasink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cf7b38-3fb7-4dad-ad5f-2415f6eb5788",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "see_results"
   },
   "outputs": [],
   "source": [
    "session.table(\"WHISPER_DEMO_OUTPUT\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558205d4-fb0a-40d7-9598-34a81f400b5d",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "select * from WHISPER_DEMO_OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72680d6-10bc-455c-b04a-4342c5271176",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": [
    "-- Create a new table to store Hindi and English translations\n",
    "CREATE OR REPLACE TABLE WHISPER_DEMO_TRANSLATION AS\n",
    "SELECT\n",
    "    *,\n",
    "    SNOWFLAKE.CORTEX.COMPLETE(\n",
    "        'openai-gpt-4.1',\n",
    "        CONCAT(\n",
    "            'Translate the following Hindi conversation to English, preserving the meaning and context as a conversation:\\n',\n",
    "            outputs\n",
    "        )\n",
    "    ) AS english_translation\n",
    "FROM WHISPER_DEMO_OUTPUT;\n",
    "\n",
    "-- View the new table\n",
    "SELECT * FROM WHISPER_DEMO_TRANSLATION;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "bharath.suresh@snowflake.com",
   "authorId": "521977325546",
   "authorName": "BSURESH",
   "lastEditTime": 1756575639764,
   "notebookId": "x2tsqoy2d35cpnxzlthp",
   "sessionId": "a57a90b1-4140-4dbf-a935-623a9b588217"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
